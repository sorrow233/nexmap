#!/usr/bin/env python3
"""
çº¯å‡€çš„ Gemini 3 Flash ç»ˆç«¯å¯¹è¯å·¥å…·
ä¸¥æ ¼æŒ‰ç…§ GMI Cloud å®˜æ–¹ç¤ºä¾‹ç¼–å†™
"""

import requests
import json
import sys

# é…ç½®ï¼ˆNative Gemini API æ ¼å¼ï¼‰
API_URL = "https://api.gmi-serving.com/v1/models/gemini-3-flash-preview:generateContent"
API_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImJmNTIyNjI5LTNkNDgtNDA0Mi04ODdkLTY4Y2ViNTRiMTJiMiIsInNjb3BlIjoiaWVfbW9kZWwiLCJjbGllbnRJZCI6IjAwMDAwMDAwLTAwMDAtMDAwMC0wMDAwLTAwMDAwMDAwMDAwMCJ9.Bm7Q34cdSXBwr0mrSLrfCq_-EbbqeeLtwoiUj5bQ3HI"

def chat(user_message, conversation_history=None):
    """
    ä½¿ç”¨ Gemini åŸç”Ÿ API ç«¯ç‚¹è¿›è¡Œå¯¹è¯
    åŸç”Ÿç«¯ç‚¹æ”¯æŒ google_search_retrieval å·¥å…·
    """
    if conversation_history is None:
        conversation_history = []
    
    # æ„å»ºåŸç”Ÿ contents ç»“æ„
    contents = []
    for msg in conversation_history:
        role = "user" if msg["role"] == "user" else "model"
        contents.append({
            "role": role,
            "parts": [{"text": msg["content"]}]
        })
    
    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    contents.append({
        "role": "user",
        "parts": [{"text": user_message}]
    })
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    
    # åŸç”Ÿ payload æ ¼å¼
    payload = {
        "contents": contents,
        "tools": [
            {
                "google_search": {}
            }
        ],
        "generationConfig": {
            "temperature": 0.7,
            "maxOutputTokens": 2000
        }
    }
    
    try:
        response = requests.post(API_URL, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()
        
        # åŸç”Ÿå“åº”è§£æ
        # data["candidates"][0]["content"]["parts"][0]["text"]
        candidate = data["candidates"][0]
        assistant_message = candidate["content"]["parts"][0]["text"]
        
        # è®°å½•å†å²
        next_history = conversation_history + [
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": assistant_message}
        ]
        
        print(f"\n[è°ƒè¯•ä¿¡æ¯ - Native API]")
        print(f"  æ¨¡å‹: google/gemini-3-flash-preview")
        if "groundingMetadata" in candidate:
            print(f"  âœ… å·²ä½¿ç”¨è”ç½‘æœç´¢æ¸²æŸ“ (Grounding used)")
            
        return assistant_message, next_history
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"å“åº”å†…å®¹: {e.response.text}")
        sys.exit(1)

def main():
    """ç»ˆç«¯å¯¹è¯ä¸»å¾ªç¯"""
    print("=" * 60)
    print("Gemini 3 Flash çº¯å‡€å¯¹è¯ç»ˆç«¯")
    print("ä¸¥æ ¼æŒ‰ç…§ GMI Cloud å®˜æ–¹ç¤ºä¾‹å®ç°")
    print("=" * 60)
    print("\nè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²\n")
    
    conversation_history = []
    
    while True:
        try:
            user_input = input("\nä½ : ").strip()
            
            if not user_input:
                continue
                
            if user_input.lower() in ['exit', 'quit']:
                print("\nå†è§ï¼ğŸ‘‹")
                break
                
            if user_input.lower() == 'clear':
                conversation_history = []
                print("\nâœ… å¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            
            # å‘é€æ¶ˆæ¯
            print("\n[æ­£åœ¨æ€è€ƒ...]")
            assistant_reply, conversation_history = chat(user_input, conversation_history)
            
            # æ˜¾ç¤ºå›å¤
            print(f"\nGemini 3: {assistant_reply}")
            
        except KeyboardInterrupt:
            print("\n\nå†è§ï¼ğŸ‘‹")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            continue

if __name__ == "__main__":
    main()
